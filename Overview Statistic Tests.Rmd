---
title: "Overview statistic tests PB"  
author: "Jonas van Nijnatten"
date: "21 november 2018"
output:
  html_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_float: true  
    toc_depth: 4
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{geometry}
- \geometry{a4paper, portrait, margin=.75in}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


contact: J.J.vanNijnatten@uva.nl

source code: https://github.com/jonasvannijnatten/R_Data_Visualization  

***

\newpage

#   Software  
## Versies  
software versions used for this tutorial:  
 - `r R.Version()$version.string`  
 - car-package version: `r packageVersion("car")` (`r packageDate("car")`)  
 - ez-package version:  `r packageVersion("ez")` (`r packageDate("ez")`)
 
##  Installatie
Benodigde packages downloaden & installeren:
```{r, eval = FALSE, echo=TRUE, results='hide', message=FALSE}
install.packages(pkgs="kableExtra",   repos="https://www.freestatistics.org/cran/")
install.packages(pkgs="car",   repos="https://www.freestatistics.org/cran/") 
install.packages(pkgs="ez",    repos="https://www.freestatistics.org/cran/")
```

Benodigde packages activeren:
```{r, eval=TRUE, message=FALSE}
library(package=kableExtra)
library(package=car)
library(package=ez)
```

***

\newpage

#  T-test

## Independent samples T-test
<details><summary><span style="color:dodgerBlue">Show code for data generation </span></summary>
```{r}
# generate data
N = 40
data.long = data.frame(
  ID = 1:N,
  condition = rep(x = c("A","B"), each = N),
  score = c(rnorm(n = N, mean = 25, sd = 6.5), rnorm(n = N, mean = 35, sd = 6.5))
)
```
</details>

```{r,echo=FALSE}
kable(data.long[c(1:5,41:45),], row.names = FALSE, caption = 'data.long') %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left")
```

Test assumption of normality
```{r}
by(data = data.long$score, INDICES = data.long$condition, FUN = shapiro.test)
```
Test assumption of equality of variances
```{r}
leveneTest(y = data.long$score, group = data.long$condition)
```
**T-test**
```{r}
t.test(formula = score ~ condition, data = data.long, paired=FALSE, alternative="two.sided", var.equal=TRUE)
```

## Paired samples T-test
<details><summary><span style="color:dodgerBlue">Show code for data generation </span></summary>
```{r}
# generate data
N = 30
data.long = data.frame(
  ID = rep(1:N,2),
  condition = rep(x = c("A","B"), each = N),
  score = c(rnorm(n = N, mean = 25, sd = 6.5), rnorm(n = N, mean = 35, sd = 6.5))
)
```
</details>

```{r,echo=FALSE}
kable(data.long[c(1:5,31:35),], row.names = FALSE, caption = 'data.long') %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left")
```

Test assumption of normality
```{r}
# calculate difference scores
diffScore = data.long$score[data.long$condition=="A"] - data.long$score[data.long$condition=="B"]
shapiro.test(diffScore)
```

**T-test**
```{r}
t.test(formula = score ~ condition, data = data.long, paired=TRUE, alternative="two.sided", var.equal=TRUE)
```

***
\newpage

#  Correlation

<details><summary><span style="color:dodgerBlue">Show code for data generation </span></summary> 
```{r}
# generate data
set.seed(05)
nrobs = 100
experience = rnorm(n = nrobs, mean = 15, sd = 3)
salary     = 10000 + ( 5 * experience ) + rnorm(n = nrobs, mean = 0, sd = 100)
data.long = data.frame(experience, salary)
# calculate correlation coefficient r
corr_coef = cor(x = data.long$experience, y = data.long$salary)
rm(list = c("nrobs", "experience", "salary"))
```
</details>   

```{r,echo=FALSE}
kable(data.long[1:5,], row.names = FALSE, caption = 'data.long') %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left")
```
Test assumption of normality
```{r}
apply(X = data.long, MARGIN = 2, FUN = shapiro.test)
```

**Pearson correlation**
```{r}
cor.test(x=data.long$experience, y=data.long$salary, alternative = "two.sided", method = "pearson")
```

**Spearman correlation**
```{r}
cor.test(x=data.long$experience, y=data.long$salary, alternative = "two.sided", method = "spearman")
```
 ***
 
\newpage 

# Regression

<details><summary><span style="color:dodgerBlue">Show code for data generation </span></summary> 
```{r}
# generate data
set.seed(05)
nrobs = 100
experience = rnorm(n = nrobs, mean = 15, sd = 3)
salary     = 10000 + ( 5 * experience ) + rnorm(n = nrobs, mean = 0, sd = 100)
data.long = data.frame(experience, salary)
# calculate correlation coefficient r
corr_coef = cor(x = data.long$experience, y = data.long$salary)
rm(list = c("nrobs", "experience", "salary"))
```
</details>   

```{r,echo=FALSE}
kable(data.long[1:5,], row.names = FALSE, caption = 'data.long') %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left")
```

Fit linear model
```{r}
linearModel = lm(formula = salary ~ experience, data = data.long)
```

Test assumption of normality & equal variances
```{r}
# plot the residuals and look at the distribution around the 0-line and if the spread is equal over all x's.
plot(x=linearModel$fitted.values, y=linearModel$residuals, xlab = "predicted", ylab = "residuals"); abline(h=0)
shapiro.test(linearModel$residuals)
```
**Regression test**
```{r}
summary(linearModel)
```


***  
\newpage

# Logistic regression

generate dataset
```{r}
set.seed(01)
N = 30
data.long = data.frame(
  x = 1:N,
  y = round( 1 / (1 + exp(-.23*(1:N-10)))  +  # logistic function of x
               rnorm(n = N, mean = 0, sd = .3) # add some noise
             ) # round it to either 1 or 0
  )
```

```{r}
# fit a logistic model, which is a generalized linear model, hence glm() function
glm.fit <- glm(y ~ x, data = data.long, family = binomial('logit'))
# test the logistic model against the NULL model
anova(glm.fit, test="Chisq")
```
```{r}
plot(data.long$x, data.long$y) # plot the measured data
lines(formula = glm.fit$fitted.values ~ data.long$x) # plot the fitted model
```

***  
\newpage

#  One-way independent samples ANOVA
<details><summary><span style="color:dodgerBlue">Toon code om voorbeeld data te genereren </span></summary>
```{r, echo=TRUE}
set.seed(05)   # set seed
nrofconds = 3  # set number of conditions
nrofsubs  = 20 # set number of subjects
subj = as.factor(1:(nrofsubs*nrofconds))      # create array with subject IDs
condition = as.factor(rep(LETTERS[1:nrofconds],each=nrofsubs))   # create array with condition values
score = as.vector( replicate(
          nrofconds , rnorm(n = nrofsubs, mean = sample(8,1)+10 , sd = sample(5,1) ) 
        ) )                                     # create array with measurement values
data.long = data.frame(subj, condition, score);      # combine arrays into a data.frame
rm(list=c("subj", "condition","score","nrofconds","nrofsubs")) # delete unnecessary variables

```   
</details>  
   
```{r,echo=FALSE}
kable(data.long[c(1:2,21:22,41:42),], row.names = FALSE, caption = 'data.long') %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left")
```


Test assumption of normality
```{r}
by(data = data.long$score, INDICES = data.long$condition, FUN = shapiro.test)
```
Test equality of variances
```{r}
leveneTest(y = data.long$score, group = data.long$condition)
```

**ANOVA*** with the aov method
```{r}
myModel = aov(formula = score ~ condition, data = data.long)
summary(myModel)
```
**ANOVA*** with the linear model method
```{r}
myModel = lm(formula = score ~ condition, data = data.long)
summary(myModel)
```

***  
\newpage

#  Factorial independent samples ANOVA

<details><summary><span style="color:dodgerBlue">Show code for data generation</span></summary>
```{r, echo=TRUE}
set.seed(01)   # set seed
nrofcondsf1 = 3  # set number of conditions for factor 1
nrofcondsf2 = 2  # set number of conditions for factor 2
nrofsubs    = nrofcondsf1*nrofcondsf2*30 # set number of subjects per condition
subj = as.factor(1:(nrofsubs))      # create array with subject IDs
# create array witht treatment conditions
treatment = as.factor(rep(LETTERS[1:nrofcondsf1],each=nrofsubs/nrofcondsf1))   
# create array with control / experimental
control   = as.factor(rep(c("control","experimental"),times=nrofsubs/nrofcondsf2))   
# create array with measurement values
score = as.vector( replicate(nrofcondsf1, replicate ( 
          nrofcondsf2 , rnorm(
            n = (nrofsubs/(nrofcondsf1*nrofcondsf2)), 
            mean = 0 , sd = sample(5,1) ) + sample(8,1)+10
        ) ) )                             
# combine arrays into a data.frame
data.long = data.frame(subj, score, treatment, control);      
# delete unnecessary arrays
rm(list=c("control","nrofcondsf1","nrofcondsf2","nrofsubs","score","subj","treatment"))

```   
</details>     

```{r,echo=FALSE}
kable(data.long[c(1:2,61:62,121:122),], row.names = FALSE, caption = 'data.long') %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left")
```

Fit model with multiple prediction factors
```{r}
model = lm(formula = score ~ treatment+control, data = data.long)
```

Test assumption of normality
```{r, eval=FALSE}
hist(model$residuals)
plot(x = fitted(model), y = residuals(model), xlab="predicted", ylab="residuals"); abline(h=0)
by(data = data.long$score, INDICES = paste(data.long$treatment, data.long$control), FUN = shapiro.test)
```
<details><summary><span style="color:dodgerBlue">Show residuals histogram</span></summary>
```{r, echo=FALSE}
hist(model$residuals)
```
</details>
<details><summary><span style="color:dodgerBlue">Show residuals vs predicted plot</span></summary>
```{r, echo=FALSE}
plot(x = fitted(model), y = residuals(model), xlab="predicted", ylab="residuals"); abline(h=0)
```
</details>
<details><summary><span style="color:dodgerBlue">Show output of Shapiro-Wilk test</span></summary>
```{r, echo=FALSE}
by(data = data.long$score, INDICES = paste(data.long$treatment, data.long$control), FUN = shapiro.test)
```
</details>

Test equality of variances
```{r}
leveneTest(y = data.long$score, group = as.factor(paste(data.long$treatment, data.long$control)))
```

**ANOVA** with the linear model method
```{r}
model = lm(formula = score ~ treatment+control, data = data.long)
library(car)
Anova(mod = model, type = 'II')
summary(model)
```
\newpage 

<details><summary><span style="color:dodgerBlue">Show code for data generation </span></summary>
```{r, echo=TRUE}
set.seed(01)   # set seed
nrofcondsf1 = 3  # set number of conditions for factor 1
nrofcondsf2 = 2  # set number of conditions for factor 2
nrofsubs    = nrofcondsf1*nrofcondsf2*20 # set number of subjects per condition
subj = as.factor(1:(nrofsubs))      # create array with subject IDs
# create array witht treatment conditions
treatment = as.factor(rep(LETTERS[1:nrofcondsf1],each=nrofsubs/nrofcondsf1))   
# create array with control / experimental
control   = as.factor(rep(c("control","experimental"),times=nrofsubs/nrofcondsf2))   
# create array with measurement values
score = as.vector( replicate(nrofcondsf1, replicate ( 
          nrofcondsf2 , rchisq(
            n = (nrofsubs/(nrofcondsf1*nrofcondsf2)), 
            df = 3)  
        )+  sample(14,1)+10  ) )                             
# combine arrays into a data.frame
data.long = data.frame(subj, score, treatment, control);      
# delete unnecessary arrays
rm(list=c("control","nrofcondsf1","nrofcondsf2","nrofsubs","score","subj","treatment"))
```
</details>   


***  
\newpage  

#  One-way repeated measures ANOVA
<details><summary><span style="color:dodgerBlue">Show code for data generation </span></summary>
```{r, echo=TRUE}
# Generate dataset
set.seed(01)   # set seed
nrofsubs  = 20 # set number of subjects
data.wide = data.frame(
  subj = as.factor(1:nrofsubs)   ,
  A =  rnorm(n = nrofsubs, mean = sample(8,1)+10 , sd = sample(5,1)) ,
  B =  rnorm(n = nrofsubs, mean = sample(8,1)+10 , sd = sample(5,1)) ,
  C =  rnorm(n = nrofsubs, mean = sample(8,1)+10 , sd = sample(5,1))
)
rm(list=c("nrofsubs")) # delete arrays
```
</details>  

```{r,echo=FALSE}
kable(data.wide[1:4,], row.names = FALSE, caption = 'data.wide') %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left")
```

fit linear model
```{r}
model = lm(formula = cbind(data.wide$A, data.wide$B, data.wide$C)~1)
```  
Test assumption of normality
```{r}
hist(model$residuals)
shapiro.test(model$residuals)
```
Test assumption of sphericity
```{r}
mauchly.test(model, X=~1)
```
ANOVA
```{r}
anova(model, X = ~1, test="Spherical")
```


***  

\newpage

#  Factorial repeated measures ANOVA

<details><summary><span style="color:dodgerBlue">Show code for data generation </span></summary>
```{r, echo=TRUE}
set.seed(02)  # set seed
nrofsubs    = 20 # set number of subjects 

data.wide = data.frame(
  ID=1:nrofsubs,
  preA=rnorm(n=nrofsubs, mean = 09,sd = 3),
  preB=rnorm(n=nrofsubs, mean = 10,sd = 3),
  preC=rnorm(n=nrofsubs, mean = 11,sd = 3),
  postA=rnorm(n=nrofsubs, mean = 11,sd = 3),
  postB=rnorm(n=nrofsubs, mean = 15,sd = 3),
  postC=rnorm(n=nrofsubs, mean = 20,sd = 3)
)
rm(list="nrofsubs")
```
</details>   


```{r,echo=FALSE}
kable(data.wide[1:4,], row.names = FALSE, caption = 'data.wide') %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left")
```

Fit linear model to the data, not yet seperating both independent factors:
```{r}
model = lm(cbind(data.wide$preA,
                 data.wide$preB,
                 data.wide$preC,
                 data.wide$postA,
                 data.wide$postB,
                 data.wide$postC
                 )~1
           )
```
Make a factor matrix clarifying the conditions. Name the colomns as the factors.
Use the parts of the column names of the data.frame in the wide format as variable values in the matrix.
```{r}
factorMatrix = data.frame(
  time=as.factor(rep(c("pre","post"),each=3)),
  condition = as.factor(rep(c("A","B","C"),2))
)
```
fit linear model specifying the two independent variables using the factorMatrix
```{r}
library(car)
factorialAnova = Anova(mod=model, idata=factorMatrix, idesign= ~time*condition, type='III')
```
run the factorial ANOVA 
```{r}
summary(object = factorialAnova, multivariate=FALSE)
```

For post-hoc analysis the data has to be transformed to the **long format**
<details><summary><span style="color:dodgerBlue">Show code for reshaping </span></summary>
```{r, results='hide'}
# reshape into wide format
data.mix = reshape(data = data.wide
                   , direction = 'long'
                   , varying = list(c('preA','preB','preC'),c('postA','postB','postC'))
                   , timevar = 'treatment'
                   , times = c("A","B","C")
                   , v.names = c("pre","post")
                   , idvar = "ID"
                   )
data.long = reshape(data = data.mix
                   , direction = 'long'
                   , varying = c("pre","post")
                   , timevar = 'pre_post'
                   , times = c("pre","post")
                   , v.names = "score"
                   , idvar = c("ID", "treatment")
                   )
```
</details>

When there are one or more significant main-effects but no significant interaction effect post-hoc comparisons can be made for each factor separately using the pairwise.t.test() function.

Post-hoc comparison for the first factor:
```{r}
pairwise.t.test(
  x = data.long$score               # column name of the dependent variable
  ,g = data.long$treatment          # column name of the first within-subjects variable
  , paired = TRUE                   # since it is a paired design, set paired=TRUE
  , p.adjust.method = "bonferroni"  # use bonferroni corrected p-values
  )
```

Post-hoc comparison for the first factor:
```{r}
pairwise.t.test(
  x = data.long$score               # column name of the dependent variable
  ,g = data.long$pre_post           # column name of the second within-subjects variable
  , paired = TRUE                   # since it is a paired design, set paired=TRUE
  , p.adjust.method = "bonferroni"  # use bonferroni corrected p-values
  )
```

When the interaction effect turns out significant, pairwise comparisons can be made between all combinations of conditions.

Post-hoc comparison for the interaction effect:
```{r}
pairwise.t.test(
  x = data.long$score               # column name of the dependent variable
  # merge both factor columns into a combined factor
  ,g = paste(data.long$treatment, data.long$pre_post)           
  , paired = TRUE                   # since it is a paired design, set paired=TRUE
  , p.adjust.method = "bonferroni"  # use bonferroni corrected p-values
  )
```

***  

\newpage

# Mixed-Design ANOVA

<details><summary><span style="color:dodgerBlue">Show code for data generation</span></summary>
```{r, echo=TRUE}
set.seed(02)   # set seed
nrofcondsf1 = 3  # set number of conditions for factor 1
nrofcondsf2 = 2  # set number of conditions for factor 2
nrofsubs    = 30 # set number of subjects per condition
subj = as.factor(rep(x= 1:(nrofsubs*nrofcondsf2), times=nrofcondsf1))       # create array with subject IDs
# create array witht treatment conditions
treatment = as.factor(rep(LETTERS[1:nrofcondsf1],each=nrofsubs*nrofcondsf2))   
# create array with control / experimental
gender   = as.factor(rep(c("men","women"),times=nrofsubs*nrofcondsf1))   
# create array with measurement values
score = as.vector( replicate(nrofcondsf1, replicate ( 
          nrofcondsf2 , rnorm(
            n = (nrofsubs), 
            mean = 0 , sd = sample(5,1) ) + sample(8,1)+10
        ) ) )                             
# combine arrays into a data.frame
data.long = data.frame(subj, score, treatment, gender);      
# delete unnecessary arrays
rm(list=c("gender","nrofcondsf1","nrofcondsf2","nrofsubs","score","subj","treatment"))

```   
</details>     

```{r,echo=FALSE}
kable(data.long[c(1:2,61:62,121:122),], row.names = FALSE, caption = 'data.long') %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE, position = "left")
```

Fit mixed-model using ezANOVA() function
```{r}
mix.model = ezANOVA(data = data.long      # name of the data frame (long format)
                    , dv = score          # column name of the dependent variabele
                    , wid = subj          # column name indicating subject ID
                    , within = treatment  # column name of the within factor
                    , between = gender    # column name of the between factor
                    , return_aov = TRUE   # add information to the output for normality testing
                    )
```

Test assumption of normality of the residuals
```{r, echo=TRUE, eval=FALSE, results='hide'}
shapiro.test(mix.model$aov$`subj:treatment`$residuals)

hist(mix.model$aov$`subj:treatment`$residuals
     , xlab = 'Residuals'
     , main = 'Histogram of residuals')
```
```{r, echo=FALSE, eval=TRUE}
shapiro.test(mix.model$aov$`subj:treatment`$residuals)
hist(mix.model$aov$`subj:treatment`$residuals, xlab = 'Residuals', main = 'Histogram of residuals')
```

Test assumption of sphericity
```{r}
mix.model$`Mauchly's Test for Sphericity`
```

If the assumption of sphericity is met use the normal, uncorrected ANOVA output
```{r}
mix.model$ANOVA
```

If the assumption of sphericity is violated, use the corrected ANOVA output
(The Greenhouse-Geisser correction is more conservative than the Huynd-Feldt correction)
```{r}
mix.model$`Sphericity Corrections`
```

When there are one or more significant main-effects but no significant interaction effect post-hoc comparisons can be made using the pairwise.t.test() function.

Post-hoc comparison for the between-subjects factor:
```{r}
pairwise.t.test(
  x=data.long$score               # column name of the dependent variable
, g=data.long$gender              # column name of the independent between-subjects variable
, paired = TRUE                   # for the within factor, set paired=TRUE
, p.adjust.method = 'bonferroni'  # use bonferroni corrected p-values
)
```

Post-hoc comparison for the within-subjects factor:
```{r}
pairwise.t.test(
  x=data.long$score               # column name of the dependent variable
, g=data.long$treatment           # column name of the independent within-subjects variable
, paired = TRUE                   # for the within factor, set paired=TRUE
, p.adjust.method = 'bonferroni'  # use bonferroni corrected p-values
)
```

When the interaction-effect turns out significant, pairwise comparisons within each factor are not very informative about what the interaction looks like. Pairwise comparisons between all combinations of conditions is inappropriate when dealing with mixed designs. The best way to interpret an interaction effect is by looking at the interaction pattern in the data using descriptive statistics and visualization.

